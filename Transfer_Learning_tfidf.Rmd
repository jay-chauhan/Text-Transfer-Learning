---
title: "Transfer_Learning_Project"
output: pdf_document
---

#Libraries

```{r}
library(data.table)
library(ggplot2)
library(tm)
library(SnowballC)
```
#Read in Data and Process

In this section, we will read in all of the data, attach a category label for each data set, and combine all of these data.frames into a single data.frame. I also noticed that IDs are recycled within different categories, so I elected to make a concatenated id field composed of the id and category fields (concatenated with a pip character between them).
```{r}
# read in data

dat_bio <- read.csv('biology.csv', stringsAsFactors = F)
dat_cook <- read.csv('cooking.csv', stringsAsFactors = F)
dat_crypt <- read.csv('crypto.csv', stringsAsFactors = F)
dat_diy <- read.csv('diy.csv', stringsAsFactors = F)
dat_robot <- read.csv('robotics.csv', stringsAsFactors = F)
dat_travel <- read.csv('travel.csv', stringsAsFactors = F)

# attach a category label
dat_bio$category <- 'biology'
dat_cook$category <- 'cooking'
dat_crypt$category <- 'crypto'
dat_diy$category <- 'diy'
dat_robot$category <- 'robotics'
dat_travel$category <- 'travel'

# combine and remove from environment
dat_all <- rbind(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)
rm(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)

# duplicate id values across categories, concat id to category and we're good
print(sum(duplicated(dat_all$id)))  # 27,441

```

#Checking heads of each file
```{r}
library(dplyr)
unique(dat_all$category)
```


```{r}
#
dim(dat_all)
```

```{r}
#Merging title and content and creating a corpus

dat_all$title_content =  paste(dat_all$title, dat_all$content, sep=" ")
```


#Create a corupus of the title and content
```{r}
 # removes all html tags
 remove_html_tags <- function(htmlString) {
     return(gsub("<.*?>", "", htmlString))
 }
```


```{r}
title_content_pre=remove_html_tags(dat_all$title_content)
title_content_corpus = Corpus(VectorSource(title_content_pre))
```


#Cleaning the Title and Content corpus. Removing Punctuation, Numbers and Extra Whitspaces.
```{r}
title_content_corpus <- tm_map(title_content_corpus, content_transformer(tolower))
title_content_corpus <- tm_map(title_content_corpus, removeNumbers)
title_content_corpus <- tm_map(title_content_corpus, removePunctuation)
title_content_corpus <- tm_map(title_content_corpus, removeWords, c("the", "and", stopwords("english")))
title_content_corpus <-  tm_map(title_content_corpus, stripWhitespace)
title_content_corpus<- tm_map(title_content_corpus, stemDocument, "english")
```

```{r}
head(title_content_corpus$content)
```


#Removing sparse words

```{r}

#One may argue that in the wordcloud, words such as can, need, know do not carry too much meaning in the setting, since we know that the entire corpus is about stack echange questions.  Therefore sometimes it is necessary to use the tf–idf(term frequency–inverse document frequency) instead of the frequencies of the term as entries, tf-idf measures the relative importance of a word to a document.

title_content_dtm_tfidf <- DocumentTermMatrix(title_content_corpus, control = list(weighting = weightTfIdf))
title_content_dtm_tfidf = removeSparseTerms(title_content_dtm_tfidf, 0.995)
title_content_dtm_tfidf
```

```{r}
# The first document
inspect(title_content_dtm_tfidf[1,])
```

```{r}
library(wordcloud)
freq = data.frame(sort(colSums(as.matrix(title_content_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```
#the above looks better

#Creating a dataframec combining the tf-idf frequencies with the tags
```{r}
dat_all_new = cbind(as.matrix(title_content_dtm_tfidf))
```


```{r}
dat_all_new = data.frame(dat_all_new)
head(dat_all_new)
dat_all_new  %>% mutate_if(is.factor,as.numeric)
dat_all_new$pred_tags<-colnames(dat_all_new)[apply(dat_all_new,1,which.max)]
```


```{r}

#counting how many tfidf tags present in corresponding travel questions
x<-(dat_all_new$pred_tags[72000:87000])
length(unique(x))
acc=0
count=1
for(i in 72000:87000){
  if(x[count] %in% strsplit(tags_all[i],split = ' ')[[1]]){
    acc=acc+1
  }
}
acc/length(unique(x))
```