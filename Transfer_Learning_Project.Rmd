---
title: "Transfer_Learning_Project"
output: pdf_document
---

#Libraries

```{r}
library(data.table)
library(ggplot2)
library(tm)
library(SnowballC)
```
#Read in Data and Process

In this section, we will read in all of the data, attach a category label for each data set, and combine all of these data.frames into a single data.frame. I also noticed that IDs are recycled within different categories, so I elected to make a concatenated id field composed of the id and category fields (concatenated with a pip character between them).
```{r}
# read in data ======

dat_bio <- read.csv('biology.csv', stringsAsFactors = F)
dat_cook <- read.csv('cooking.csv', stringsAsFactors = F)
dat_crypt <- read.csv('crypto.csv', stringsAsFactors = F)
dat_diy <- read.csv('diy.csv', stringsAsFactors = F)
dat_robot <- read.csv('robotics.csv', stringsAsFactors = F)
dat_travel <- read.csv('travel.csv', stringsAsFactors = F)

# attach a category label
dat_bio$category <- 'biology'
dat_cook$category <- 'cooking'
dat_crypt$category <- 'crypto'
dat_diy$category <- 'diy'
dat_robot$category <- 'robotics'
dat_travel$category <- 'travel'

# combine and remove from environment
dat_all <- rbind(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)
rm(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)

# duplicate id values across categories, concat id to category and we're good
print(sum(duplicated(dat_all$id)))  # 27,441

```

#Checking heads of each file
```{r}
library(dplyr)
unique(dat_all$category)
```

#Cleaning - Functions to Remove HTML tags and Punctutation
```{r}
# 
# # removes all html tags
# remove_html_tags <- function(htmlString) {
#     return(gsub("<.*?>", "", htmlString))
# }
# 
# 
# # custom tokenizer
# custom_tokenizer <- function(param_big_string) {
#     #' lower-cases, removes punctuation, new line and return characters, 
#     #' and removes unnecessary whitespace, then strsplits 
#     split_content <- sapply(param_big_string, removePunctuation, preserve_intra_word_dashes=T)
#     split_content <- sapply(split_content, function(y) gsub("[\r\n]", " ", y))
#     split_content <- sapply(split_content, tolower)
#     split_content <- sapply(split_content, function(y) gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", y, perl=TRUE))
#     
#     return(split_content)
#     #return(split_content <- (sapply(split_content, strsplit, " ")))
# }
# 
# custom_preprocess<-function(text,stem=TRUE){
#   corpus <- Corpus(VectorSource(text))
#   docs <- tm_map(corpus, removeWords, stopwords("english"))
#   docs <- tm_map(docs, PlainTextDocument)
#   if(stem==TRUE){
#     docs <- tm_map(docs, stemDocument, "english")
#   }
#   return(docs$content)
# }

```

#Applying those functions on our combined dataframe

```{r}
# # quick clean on content column
# dat_all$content <- remove_html_tags(dat_all$content)
# 
# # tokenize
# dat_all$content <- custom_tokenizer(dat_all$content)
# dat_all$tags <- custom_tokenizer(dat_all$tags)
# dat_all$title <- custom_tokenizer(dat_all$title)
# 
# dat_all$content<-custom_preprocess(dat_all$content)
# dat_all$tags<-custom_preprocess(dat_all$tags,FALSE)
# dat_all$title<-custom_preprocess(dat_all$title)
```



```{r}
#
summary(dat_all)
```

```{r}
#Merging title and content and creating a corpus

dat_all$title_content =  paste(dat_all$title, dat_all$content, sep=" ")

```


#Create a corupus of the title and content
```{r}
title_content_corpus = Corpus(VectorSource(dat_all$title_content))
```


#Cleaning the Title and Content corpus. Removing Punctuation, Numbers and Extra Whitspaces.
```{r}
title_content_corpus = tm_map(title_content_corpus, content_transformer(tolower))
title_content_corpus = tm_map(title_content_corpus, removeNumbers)
title_content_corpus = tm_map(title_content_corpus, removePunctuation)
title_content_corpus = tm_map(title_content_corpus, removeWords, c("the", "and", stopwords("english")))
title_content_corpus =  tm_map(title_content_corpus, stripWhitespace)
```

#Creating the Document Term Matrix for our corpus

```{r}
title_content_dtm <- DocumentTermMatrix(title_content_corpus)
title_content_dtm
```

```{r}
inspect(title_content_dtm[500:505, 500:505])
```

#To reduce the dimension of the DTM, we can emove the less frequent terms such that the sparsity is less than 0.99
```{r}
title_content_dtm = removeSparseTerms(title_content_dtm, 0.99)
title_content_dtm
```
#The first title content looks like the following
```{r}
inspect(title_content_dtm[1,1:20])
```


```{r}
library(wordcloud)
freq = data.frame(sort(colSums(as.matrix(title_content_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
```

#One may argue that in the wordcloud, words such as can, need, know do not carry too much meaning in the setting, since we know that the entire corpus is about stack echange questions.  Therefore sometimes it is necessary to use the tf–idf(term frequency–inverse document frequency) instead of the frequencies of the term as entries, tf-idf measures the relative importance of a word to a document.

```{r}
title_content_dtm_tfidf <- DocumentTermMatrix(title_content_corpus, control = list(weighting = weightTfIdf))
title_content_dtm_tfidf = removeSparseTerms(title_content_dtm_tfidf, 0.99)
title_content_dtm_tfidf
```

```{r}
# The first document
inspect(title_content_dtm_tfidf[1,1:20])
```

```{r}
freq = data.frame(sort(colSums(as.matrix(title_content_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```
#the above looks better

#Creating a dataframec combining the tf-idf frequencies with the tags
```{r}
dat_all_new = cbind(dat_all$tags, dat_all$category, as.matrix(title_content_dtm_tfidf))
```


```{r}
dat_all_new = data.frame(dat_all_new)
```


#Wordclound for Tags

```{r}
tag_corpus = Corpus(VectorSource(dat_all$tags))
tag_corpus = tm_map(tag_corpus, content_transformer(tolower))
tag_corpus = tm_map(tag_corpus, removeNumbers)
tag_corpus = tm_map(tag_corpus, removePunctuation)
tag_corpus = tm_map(tag_corpus, removeWords, c("the", "and", stopwords("english")))
tag_corpus =  tm_map(tag_corpus, stripWhitespace)
```

```{r}
tag_dtm_tfidf <- DocumentTermMatrix(tag_corpus, control = list(weighting = weightTfIdf))
tag_dtm_tfidf = removeSparseTerms(tag_dtm_tfidf, 0.99)
tag_dtm_tfidf
```

```{r}
freq = data.frame(sort(colSums(as.matrix(tag_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```

#Converting Tags to dummy variables for binary relevance
```{r}
library(mlr)
tags_df = dat_all$tags
tags_df <- sapply(tags_df, function(x) { gsub('"','', x)})
tags_df
```

