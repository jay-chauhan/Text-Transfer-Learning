---
title: "Transfer_Learning_Project"
output: pdf_document
---

#Libraries

```{r}
library(data.table)
library(ggplot2)
library(tm)
library(SnowballC)
```
#Read in Data and Process

In this section, we will read in all of the data, attach a category label for each data set, and combine all of these data.frames into a single data.frame. I also noticed that IDs are recycled within different categories, so I elected to make a concatenated id field composed of the id and category fields (concatenated with a pip character between them).
```{r}
# read in data

dat_bio <- read.csv('biology.csv', stringsAsFactors = F)
dat_cook <- read.csv('cooking.csv', stringsAsFactors = F)
dat_crypt <- read.csv('crypto.csv', stringsAsFactors = F)
dat_diy <- read.csv('diy.csv', stringsAsFactors = F)
dat_robot <- read.csv('robotics.csv', stringsAsFactors = F)
dat_travel <- read.csv('travel.csv', stringsAsFactors = F)

# attach a category label
dat_bio$category <- 'biology'
dat_cook$category <- 'cooking'
dat_crypt$category <- 'crypto'
dat_diy$category <- 'diy'
dat_robot$category <- 'robotics'
dat_travel$category <- 'travel'

# combine and remove from environment
dat_all <- rbind(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)
rm(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)

# duplicate id values across categories, concat id to category and we're good
print(sum(duplicated(dat_all$id)))  # 27,441

```

#Checking heads of each file
```{r}
library(dplyr)
unique(dat_all$category)
```

#Cleaning - Functions to Remove HTML tags and Punctutation
```{r}
# 
# # removes all html tags
# remove_html_tags <- function(htmlString) {
#     return(gsub("<.*?>", "", htmlString))
# }
# 
# 
# # custom tokenizer
# custom_tokenizer <- function(param_big_string) {
#     #' lower-cases, removes punctuation, new line and return characters, 
#     #' and removes unnecessary whitespace, then strsplits 
#     split_content <- sapply(param_big_string, removePunctuation, preserve_intra_word_dashes=T)
#     split_content <- sapply(split_content, function(y) gsub("[\r\n]", " ", y))
#     split_content <- sapply(split_content, tolower)
#     split_content <- sapply(split_content, function(y) gsub("(?<=[\\s])\\s*|^\\s+|\\s+$", "", y, perl=TRUE))
#     
#     return(split_content)
#     #return(split_content <- (sapply(split_content, strsplit, " ")))
# }
# 
# custom_preprocess<-function(text,stem=TRUE){
#   corpus <- Corpus(VectorSource(text))
#   docs <- tm_map(corpus, removeWords, stopwords("english"))
#   docs <- tm_map(docs, PlainTextDocument)
#   if(stem==TRUE){
#     docs <- tm_map(docs, stemDocument, "english")
#   }
#   return(docs$content)
# }

```

#Applying those functions on our combined dataframe

```{r}
# # quick clean on content column
# dat_all$content <- remove_html_tags(dat_all$content)
# 
# # tokenize
# dat_all$content <- custom_tokenizer(dat_all$content)
# dat_all$tags <- custom_tokenizer(dat_all$tags)
# dat_all$title <- custom_tokenizer(dat_all$title)
# 
# dat_all$content<-custom_preprocess(dat_all$content)
# dat_all$tags<-custom_preprocess(dat_all$tags,FALSE)
# dat_all$title<-custom_preprocess(dat_all$title)
```



```{r}
#
head(dat_all)
```

```{r}
#Merging title and content and creating a corpus

dat_all$title_content =  paste(dat_all$title, dat_all$content, sep=" ")
head(dat_all)
```


#Create a corupus of the title and content
```{r}
 # removes all html tags
 remove_html_tags <- function(htmlString) {
     return(gsub("<.*?>", "", htmlString))
 }
```


```{r}
title_content_pre=remove_html_tags(dat_all$title_content)
title_content_corpus = Corpus(VectorSource(title_content_pre))
```


#Cleaning the Title and Content corpus. Removing Punctuation, Numbers and Extra Whitspaces.
```{r}
title_content_corpus <- tm_map(title_content_corpus, content_transformer(tolower))
title_content_corpus <- tm_map(title_content_corpus, removeNumbers)
title_content_corpus <- tm_map(title_content_corpus, removePunctuation)
title_content_corpus <- tm_map(title_content_corpus, removeWords, c("the", "and", stopwords("english")))
title_content_corpus <-  tm_map(title_content_corpus, stripWhitespace)
title_content_corpus<- tm_map(title_content_corpus, stemDocument, "english")
```

```{r}
head(title_content_corpus$content)
```


#Creating the Document Term Matrix for our corpus

```{r}
title_content_dtm <- DocumentTermMatrix(title_content_corpus)
title_content_dtm
```

```{r}
inspect(title_content_dtm[500:505, 500:505])
```

#To reduce the dimension of the DTM, we can emove the less frequent terms such that the sparsity is less than 0.99
```{r}
title_content_dtm = removeSparseTerms(title_content_dtm, 0.99)
title_content_dtm
```
#The first title content looks like the followin


```{r}
library(wordcloud)
freq = data.frame(sort(colSums(as.matrix(title_content_dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=50, colors=brewer.pal(1, "Dark2"))
```

#One may argue that in the wordcloud, words such as can, need, know do not carry too much meaning in the setting, since we know that the entire corpus is about stack echange questions.  Therefore sometimes it is necessary to use the tf–idf(term frequency–inverse document frequency) instead of the frequencies of the term as entries, tf-idf measures the relative importance of a word to a document.

```{r}
title_content_dtm_tfidf <- DocumentTermMatrix(title_content_corpus, control = list(weighting = weightTfIdf))
title_content_dtm_tfidf = removeSparseTerms(title_content_dtm_tfidf, 0.99)
title_content_dtm_tfidf
```

```{r}
# The first document
inspect(title_content_dtm_tfidf[1,])
```

```{r}
freq = data.frame(sort(colSums(as.matrix(title_content_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```
#the above looks better

#Creating a dataframec combining the tf-idf frequencies with the tags
```{r}
dat_all_new = cbind(as.matrix(title_content_dtm_tfidf))
head(dat_all_new)
```


```{r}
dat_all_new = data.frame(dat_all_new)
dat_all_train<-dat_all_new[1:(nrow(dat_all_new)-15000),]
dat_all_test<-tail(dat_all_new,15000)
#head(dat_all_new)
#dat_all_new  %>% mutate_if(is.factor,as.numeric)
```

#Wordclound for Tags

```{r}
tag_corpus = Corpus(VectorSource(dat_all$tags))
tag_corpus = tm_map(tag_corpus, content_transformer(tolower))
tag_corpus = tm_map(tag_corpus, stripWhitespace)
tag_corpus = tm_map(tag_corpus, removeNumbers)
tag_corpus = tm_map(tag_corpus, removePunctuation,preserve_intra_word_dashes=T)
```

```{r}
library(qdapTools)

#extract tags from the corpus in df format
tags_train<-tag_corpus$content[1:(nrow(dat_all_new)-15000)]
tags_test<-tail(tag_corpus$content,15000)
head(tags_train)

#convert tags into one-hot encoding
tags_train_ohef<-cbind(tags_train, mtabulate(strsplit(tags_train, " ")))
tags_train_ohe <- data.frame(lapply(tags_train_ohef, as.logical))

tags_test_ohef<-cbind(tags_test, mtabulate(strsplit(tags_test, " ")))
tags_test_ohe <- data.frame(lapply(tags_test_ohef, as.logical))

#column bind the featureset and outputs to create final dataset

#dat_all_train<-cbind(dat_all_train,tags_train_ohe)
#dat_all_test<-cbind(dat_all_test,tags_test_ohe)
```


```{r}
# To select only those tags that appear in 2 or more categories

xy_test<-cbind(category=dat_all$category[1:(nrow(dat_all_new)-15000)],tags_train_ohef)
xy_test1 <- as.data.frame(xy_test)
head(xy_test1)
aggregate(.~ xy_test[, 1], xy_test, sum)
xy_test$tags_train<-NULL
DT <- data.table(xy_test)
xy<-DT[, lapply(.SD,sum), by=list(category)]
hello <- which(colSums(xy != 0)> 2)
hello <- as.data.frame(hello)
#hello <- hello[2:nrow(hello),]
hello <- rownames(hello)[2:nrow(hello)]
hello
```

```{r}
# Subsetting and keeping only the columns identified in the set above. Doesn't work on test, as column names are different for that. Thus commented out.

tags_train_ohe <- tags_train_ohe[, (names(tags_train_ohe) %in% hello)]
#tags_test_ohe <- tags_test_ohe[, (names(tags_test_ohe) %in% hello)]

dat_all_train<-cbind(dat_all_train,tags_train_ohe)
#dat_all_test<-cbind(dat_all_test,tags_test_ohe)
```

```{r}
dat_all_train[, 841:857]
```


```{r}
#select 15000 travel domain questions as test set and remaining as train


#dim(dat_all_train)
#dat_all_train <- dat_all_train[ -c(841 ) ]

#shuffle train data
set.seed(1200)
dat_all_train <- dat_all_train[sample(nrow(dat_all_train)),]

dim(dat_all_train)
head(dat_all_train)[,841:857]
```

```{r}
dat_all_train['salt']
```


```{r}
library(mlr)
labels = colnames(dat_all_train[841:857])
multilabel.task = makeMultilabelTask(id = "multi", data = dat_all_train, target = labels)
#yeast.task
```


```{r}
lrn.rfsrc = makeLearner("multilabel.randomForestSRC")

lrn.br = makeLearner("classif.rpart", predict.type = "prob")
lrn.br = makeMultilabelBinaryRelevanceWrapper(lrn.br)
mod = train(lrn.br, multilabel.task)
pred = predict(mod, task = multilabel.task, subset = 1:10)

performance(pred)

getMultilabelBinaryPerformances(pred, measures = list(acc, mmce, auc))
```


```{r}
# tag_dtm_tfidf <- DocumentTermMatrix(tag_corpus)
# #tag_dtm_tfidf = removeSparseTerms(tag_dtm_tfidf, 0.99)
# #tag_dtm_tfidf
```

```{r}
# tag_dtm_tfidf$content
```

```{r}
# freq = data.frame(sort(colSums(as.matrix(tag_dtm_tfidf)), decreasing=TRUE))
# wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```




#Converting Tags to dummy variables for binary relevance
```{r}
# library(mlr)
# tags_df = dat_all$tags
# tags_df <- sapply(tags_df, function(x) { gsub('"','', x)})
# tags_df
```










