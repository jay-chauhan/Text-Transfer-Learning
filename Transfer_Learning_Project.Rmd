---
title: "Transfer_Learning_Project"
output: pdf_document
---

#Libraries

```{r}
library(data.table)
library(ggplot2)
library(tm)
library(SnowballC)
```
#Read in Data and Process

In this section, we will read in all of the data, attach a category label for each data set, and combine all of these data.frames into a single data.frame. I also noticed that IDs are recycled within different categories, so I elected to make a concatenated id field composed of the id and category fields (concatenated with a pip character between them).
```{r}
# read in data

dat_bio <- read.csv('biology.csv', stringsAsFactors = F)
dat_cook <- read.csv('cooking.csv', stringsAsFactors = F)
dat_crypt <- read.csv('crypto.csv', stringsAsFactors = F)
dat_diy <- read.csv('diy.csv', stringsAsFactors = F)
dat_robot <- read.csv('robotics.csv', stringsAsFactors = F)
dat_travel <- read.csv('travel.csv', stringsAsFactors = F)

# attach a category label
dat_bio$category <- 'biology'
dat_cook$category <- 'cooking'
dat_crypt$category <- 'crypto'
dat_diy$category <- 'diy'
dat_robot$category <- 'robotics'
dat_travel$category <- 'travel'

# combine and remove from environment
dat_all <- rbind(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)
rm(dat_bio, dat_cook, dat_crypt, dat_diy, dat_robot, dat_travel)

# duplicate id values across categories, concat id to category and we're good
print(sum(duplicated(dat_all$id)))  # 27,441

```

#Checking heads of each file
```{r}
library(dplyr)
unique(dat_all$category)
```


```{r}
#
head(dat_all)
```

```{r}
#Merging title and content and creating a corpus

dat_all$title_content =  paste(dat_all$title, dat_all$content, sep=" ")
colnames(dat_all)
```


#Create a corupus of the title and content
```{r}
 # removes all html tags
 remove_html_tags <- function(htmlString) {
     return(gsub("<.*?>", "", htmlString))
 }
```


```{r}
title_content_pre=remove_html_tags(dat_all$title_content)
title_content_corpus = Corpus(VectorSource(title_content_pre))
```


#Cleaning the Title and Content corpus. Removing Punctuation, Numbers and Extra Whitspaces.
```{r}
title_content_corpus <- tm_map(title_content_corpus, content_transformer(tolower))
title_content_corpus <- tm_map(title_content_corpus, removeNumbers)
title_content_corpus <- tm_map(title_content_corpus, removePunctuation)
title_content_corpus <- tm_map(title_content_corpus, removeWords, c("the", "and", stopwords("english")))
title_content_corpus <-  tm_map(title_content_corpus, stripWhitespace)
title_content_corpus<- tm_map(title_content_corpus, stemDocument, "english")
```

```{r}
head(title_content_corpus$content)
```


#Removing sparse words

```{r}

#One may argue that in the wordcloud, words such as can, need, know do not carry too much meaning in the setting, since we know that the entire corpus is about stack echange questions.  Therefore sometimes it is necessary to use the tf–idf(term frequency–inverse document frequency) instead of the frequencies of the term as entries, tf-idf measures the relative importance of a word to a document.

title_content_dtm_tfidf <- DocumentTermMatrix(title_content_corpus, control = list(weighting = weightTfIdf))
title_content_dtm_tfidf = removeSparseTerms(title_content_dtm_tfidf, 0.99)
title_content_dtm_tfidf
```

```{r}
# The first document
inspect(title_content_dtm_tfidf[1,])
```

```{r}
library(wordcloud)
freq = data.frame(sort(colSums(as.matrix(title_content_dtm_tfidf)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words=100, colors=brewer.pal(1, "Dark2"))
```
#the above looks better

#Creating a dataframec combining the tf-idf frequencies with the tags
```{r}
dat_all_new = cbind(as.matrix(title_content_dtm_tfidf))
```


```{r}
dat_all_new = data.frame(dat_all_new)
dim(dat_all_new)
dat_all_new  %>% mutate_if(is.factor,as.numeric)
```

#Working with Tags. 
1. Creating Tag Corpus and processing text

```{r}
tag_corpus = Corpus(VectorSource(dat_all$tags))
tag_corpus = tm_map(tag_corpus, content_transformer(tolower))
tag_corpus = tm_map(tag_corpus, stripWhitespace)
tag_corpus = tm_map(tag_corpus, removeNumbers)
tag_corpus = tm_map(tag_corpus, removePunctuation,preserve_intra_word_dashes=T)
```



```{r}
library(qdapTools)

#extract tags from the corpus in df format
tags_all = (tag_corpus$content[1:(nrow(dat_all_new))])
head(tags_all)

#convert tags into one-hot encoding
tags_binary = cbind(tags_all, mtabulate(strsplit((tags_all), " ")))

#tags_test_ohef<-cbind(tags_test, mtabulate(strsplit(tags_test, " ")))
#tags_test_ohe <- data.frame(lapply(tags_test_ohef, as.logical))

#column bind the featureset and outputs to create final dataset
dat_all_feature_df <-cbind(dat_all_new,tags_binary)
dim(dat_all_feature_df)
```


```{r}
# To select only those tags that appear in 2 or more categories

#1. Appending Tags to the actual Dataframe
cat_tag_df = data.frame(cbind(category = dat_all$category, tags_binary))
#Removing tags_all column
cat_tag_df$tags_all = NULL
dim(cat_tag_df)

aggregate(.~ cat_tag_df[, 1], cat_tag_df, sum)
#xy_test$tags_train<-NULL
cat_tag_dt <- data.table(cat_tag_df)
dim(cat_tag_dt)

#Grouping by Category
cat_tag_by_catogory <-cat_tag_dt[, lapply(.SD,sum), by=list(category)]
head(cat_tag_by_catogory)

#Filtering Tags which appear in more than 2 categories
more_than_2_cat_tags <- which(colSums(cat_tag_by_catogory != 0)> 2)
more_than_2_cat_tags
more_than_2_cat_tags_df <- as.data.frame(more_than_2_cat_tags)
dim(more_than_2_cat_tags_df)
#hello <- hello[2:nrow(hello),]
more_than_2_cat_tags_df <- rownames(more_than_2_cat_tags_df)[2:nrow(more_than_2_cat_tags_df)]

#Removing untagged from list
more_than_2_cat_tags_list <- more_than_2_cat_tags_df[more_than_2_cat_tags_df != "untagged"]
more_than_2_cat_tags_list <- more_than_2_cat_tags_df[more_than_2_cat_tags_df != "salt"]
length(more_than_2_cat_tags_list)
more_than_2_cat_tags_list
```

#Filtering DataFrame with only these tags 
```{r}
# Subsetting and keeping only the tag columns identified in the set above.
tags_binary <- tags_binary[, (names(tags_binary)) %in% more_than_2_cat_tags_list]

dat_all_feature_new_df <-cbind(dat_all_new,tags_binary)
```

#Splitting Data into Train and Test
```{r}
#Training Set
dat_all_train = dat_all_feature_new_df[1:(nrow(dat_all_feature_new_df)-15000),]
dim(dat_all_train)
dat_x_train = dat_all_train[, -c(841:856)]
dim(dat_x_train)
head(dat_x_train)
dat_y_train = dat_all_train[, 841:856]
dim(dat_y_train)
head(dat_y_train)
```



```{r}
#Test Set
dat_all_test<-tail(dat_all_feature_new_df,15000)
dim(dat_all_test)
dat_x_test = dat_all_test[, -c(841:856)]
dim(dat_x_test)
head(dat_x_train)
dat_y_test = dat_all_test[, 841:856]
dim(dat_y_test)
head(dat_y_test)
```
#Preparing Data For mlr
```{r}
#Combining Train and test data
data_all = rbind(dat_x_train, dat_x_test)
target_all = rbind(dat_y_train, dat_y_test)

#Target should be encoded as logical. 
target_all <- data.frame(lapply(target_all, as.logical))
head(target_all)
```

#Creating an MLR Task


```{r}
set.seed(1729)
library(mlr)
labels = c()
for (i in (1:length(colnames(target_all)))){
  labels[i] = (paste0("label",i))
}

features = c()
for (i in (1:length(colnames(data_all)))){
  features[i] = (paste0("x",i))
}


colnames(data_all) = features
colnames(target_all) = labels

#Data with Target
dat_all_with_target = cbind(data_all,target_all)

#Creating Task
tags.task = makeMultilabelTask(data = dat_all_with_target, target = labels)

```


#Creating a Binary Learner - Classifier Chains
```{r}
binary.learner = makeLearner("classif.rpart")
lrncc = makeMultilabelClassifierChainsWrapper(binary.learner)
```

#Train and Predict
```{r}
n = getTaskSize(tags.task)
n
train.set = 1:72000
test.set = 72000:87000
```



```{r}
tags.mod.cc = train(lrncc, tags.task, subset = train.set)
tags.pred.cc = predict(tags.mod.cc, task = tags.task, subset = test.set)
```

#Performance
```{r}
performance(tags.pred.cc, measures = list(multilabel.hamloss, multilabel.subset01, multilabel.f1, multilabel.acc))
```



```{r}
getMultilabelBinaryPerformances(tags.pred.cc, measures = list(acc, mmce))
```

#Binary Relevance
```{r}
lrn.br = makeLearner("classif.rpart", predict.type = "prob")
lrn.br = makeMultilabelBinaryRelevanceWrapper(lrn.br)
lrn.br
```


```{r}
tags.mod.br = train(lrn.br, tags.task, subset = train.set)
tags.pred.br = predict(tags.mod.br, task = tags.task, subset = test.set)
```



```{r}
getMultilabelBinaryPerformances(tags.pred.br, measures = list(acc, mmce))
```

```{r}
predictions_df = data.frame(tags.pred.br)
colnames(predictions_df) = colnames(target_all)
head(predictions_df)
predictions_df %>% filter(response.label16 == TRUE)

#2, 9 - Children and Safety
```

```{r}
colnames(dat_y_test)
```

